{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Streaming application using Spark Structured Streaming  \n",
    "In this task, you will implement Spark Structured Streaming to consume the data from task 1 and perform prediction.  \n",
    "  \n",
    "Important:   \n",
    "-\tThis task uses PySpark Structured Streaming with PySpark Dataframe APIs and PySpark ML.  \n",
    "-\tYou also need your pipeline model from A2A to make predictions and persist the results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write code to create a SparkSession, which 1) uses four cores with a proper application name; 2) use the Melbourne timezone; 3) ensure a checkpoint location has been set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StreamingApp_A2B\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Australia/Melbourne\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StreamingApp_A2B\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Australia/Melbourne\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/spark_checkpoint\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets (e.g. restaurants, delivery_address) into data frames. (You can reuse your code from 2A.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delivery Address Schema:\n",
      "root\n",
      " |-- gid: integer (nullable = true)\n",
      " |-- street_name: string (nullable = true)\n",
      " |-- street_type: string (nullable = true)\n",
      " |-- suburb: string (nullable = true)\n",
      " |-- postcode: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- geom: string (nullable = true)\n",
      " |-- delivery_id: integer (nullable = true)\n",
      "\n",
      "Sample Data:\n",
      "+-----+-----------+-----------+---------------+--------+-----+------------+------------+--------------------+-----------+\n",
      "|  gid|street_name|street_type|         suburb|postcode|state|    latitude|   longitude|                geom|delivery_id|\n",
      "+-----+-----------+-----------+---------------+--------+-----+------------+------------+--------------------+-----------+\n",
      "|17985| KENSINGTON|       ROAD|     KENSINGTON|    3031|  VIC|-37.79431483|144.92699295|POINT (144.926992...|          1|\n",
      "|28383|    RAILWAY|      PLACE| WEST MELBOURNE|    3003|  VIC| -37.8094632|144.94475838|POINT (144.944758...|          2|\n",
      "| 8781|   COURTNEY|     STREET|NORTH MELBOURNE|    3051|  VIC|-37.80076484|144.95209394|POINT (144.952093...|          3|\n",
      "|13455|    FERRARS|     STREET|SOUTH MELBOURNE|    3205|  VIC|-37.83289984|144.95463694|POINT (144.954636...|          4|\n",
      "| 8333|    COLLINS|     STREET|      MELBOURNE|    3000|  VIC|-37.81471878|144.96996338|POINT (144.969963...|          5|\n",
      "+-----+-----------+-----------+---------------+--------+-----+------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Driver Schema:\n",
      "root\n",
      " |-- driver_id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      " |-- year_experience: integer (nullable = true)\n",
      " |-- vehicle_condition: string (nullable = true)\n",
      " |-- type_of_vehicle: string (nullable = true)\n",
      "\n",
      "Sample Data:\n",
      "+---------+---+------+---------------+-----------------+---------------+\n",
      "|driver_id|age|rating|year_experience|vehicle_condition|type_of_vehicle|\n",
      "+---------+---+------+---------------+-----------------+---------------+\n",
      "|     1001| 49|   3.9|              3|             Good|           Bike|\n",
      "|     1002| 30|   3.9|              1|             Poor|        Scooter|\n",
      "|     1003| 48|   3.7|              5|             Poor|        Scooter|\n",
      "|     1004| 29|   3.5|              4|        Excellent|          eBike|\n",
      "|     1005| 25|   4.9|              3|             Poor|           Bike|\n",
      "+---------+---+------+---------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Restaurant Schema:\n",
      "root\n",
      " |-- row_id: integer (nullable = true)\n",
      " |-- restaurant_code: string (nullable = true)\n",
      " |-- chain_id: string (nullable = true)\n",
      " |-- primary_cuisine: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- geom: string (nullable = true)\n",
      " |-- restaurant_id: integer (nullable = true)\n",
      " |-- suburb: string (nullable = true)\n",
      " |-- postcode: integer (nullable = true)\n",
      "\n",
      "Sample Data:\n",
      "+------+---------------+--------+---------------+------------+------------+--------------------+-------------+---------------+--------+\n",
      "|row_id|restaurant_code|chain_id|primary_cuisine|    latitude|   longitude|                geom|restaurant_id|         suburb|postcode|\n",
      "+------+---------------+--------+---------------+------------+------------+--------------------+-------------+---------------+--------+\n",
      "| 19190|       615b5e7e|f9aa47f6|       Japanese|-37.80092933|144.95515785|POINT (144.955157...|          983|NORTH MELBOURNE|    3051|\n",
      "|  9352|       06e9e0cd|31300311|       Japanese|-37.83857784|144.94635494|POINT (144.946354...|          982| PORT MELBOURNE|    3207|\n",
      "| 17725|       32b66420|ec3b749b|         Indian|-37.79467493|144.96772917|POINT (144.967729...|          977|        CARLTON|    3053|\n",
      "|  9962|       64a03960|b6620d07|       Japanese|-37.77962313|144.94087653|POINT (144.940876...|          997|      PARKVILLE|    3052|\n",
      "| 22873|       c873a50b|9f52af27|      Beverages|-37.85058184|144.99447093|POINT (144.994470...|          985|        PRAHRAN|    3181|\n",
      "+------+---------------+--------+---------------+------------+------------+--------------------+-------------+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "New Order Schema:\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- delivery_person_id: integer (nullable = true)\n",
      " |-- order_ts: long (nullable = true)\n",
      " |-- ready_ts: long (nullable = true)\n",
      " |-- weather_condition: string (nullable = true)\n",
      " |-- road_condition: string (nullable = true)\n",
      " |-- type_of_order: string (nullable = true)\n",
      " |-- order_total: integer (nullable = true)\n",
      " |-- delivery_time: integer (nullable = true)\n",
      " |-- travel_distance: float (nullable = true)\n",
      " |-- restaurant_id: integer (nullable = true)\n",
      " |-- delivery_id: integer (nullable = true)\n",
      "\n",
      "Sample Data:\n",
      "+--------------------+------------------+----------+----------+-----------------+--------------+-------------+-----------+-------------+---------------+-------------+-----------+\n",
      "|            order_id|delivery_person_id|  order_ts|  ready_ts|weather_condition|road_condition|type_of_order|order_total|delivery_time|travel_distance|restaurant_id|delivery_id|\n",
      "+--------------------+------------------+----------+----------+-----------------+--------------+-------------+-----------+-------------+---------------+-------------+-----------+\n",
      "|88a5d521-375d-483...|              1547|1736286720|1736287104|           Cloudy|           Jam|       Drinks|         17|           36|            5.5|            2|       3184|\n",
      "|b6375ca0-a6a8-42d...|              1159|1736194432|1736194432|            Sunny|          High|         Meal|         36|            5|            2.5|          984|       8973|\n",
      "|519df007-4531-4e7...|              1343|1735950720|1735950848|            Windy|        Medium|       Drinks|         14|           13|            7.5|          650|       4711|\n",
      "|5cda6b3b-b3a9-491...|              1966|1736873088|1736873216|            Sunny|          High|        Combo|         77|           21|            4.5|          422|       2174|\n",
      "|01dfc166-9567-4cc...|              1887|1736216704|1736216960|            Foggy|           Jam|       Drinks|          8|           59|           10.5|          657|       6596|\n",
      "+--------------------+------------------+----------+----------+-----------------+--------------+-------------+-----------+-------------+---------------+-------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, LongType, FloatType\n",
    "# Delivery Address Schema\n",
    "delivery_address_schema = StructType([\n",
    "    StructField(\"gid\", IntegerType(), True),\n",
    "    StructField(\"street_name\", StringType(), True),\n",
    "    StructField(\"street_type\", StringType(), True),\n",
    "    StructField(\"suburb\", StringType(), True),\n",
    "    StructField(\"postcode\", IntegerType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"geom\", StringType(), True),\n",
    "    StructField(\"delivery_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Driver Schema\n",
    "driver_schema = StructType([\n",
    "    StructField(\"driver_id\", IntegerType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"rating\", FloatType(), True),\n",
    "    StructField(\"year_experience\", IntegerType(), True),\n",
    "    StructField(\"vehicle_condition\", StringType(), True),\n",
    "    StructField(\"type_of_vehicle\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Restaurant Schema\n",
    "restaurant_schema = StructType([\n",
    "    StructField(\"row_id\", IntegerType(), True),\n",
    "    StructField(\"restaurant_code\", StringType(), True),\n",
    "    StructField(\"chain_id\", StringType(), True),\n",
    "    StructField(\"primary_cuisine\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"geom\", StringType(), True),\n",
    "    StructField(\"restaurant_id\", IntegerType(), True),\n",
    "    StructField(\"suburb\", StringType(), True),\n",
    "    StructField(\"postcode\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# New Order Schema (instead of Order Schema)\n",
    "new_order_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"delivery_person_id\", IntegerType(), True),\n",
    "    StructField(\"order_ts\", LongType(), True),\n",
    "    StructField(\"ready_ts\", LongType(), True),\n",
    "    StructField(\"weather_condition\", StringType(), True),\n",
    "    StructField(\"road_condition\", StringType(), True),\n",
    "    StructField(\"type_of_order\", StringType(), True),\n",
    "    StructField(\"order_total\", IntegerType(), True),\n",
    "    StructField(\"delivery_time\", IntegerType(), True),\n",
    "    StructField(\"travel_distance\", FloatType(), True),\n",
    "    StructField(\"restaurant_id\", IntegerType(), True),\n",
    "    StructField(\"delivery_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Load Static Datasets into DataFrames\n",
    "delivery_address_df = spark.read.csv(\"delivery_address.csv\", schema=delivery_address_schema, header=True)\n",
    "driver_df = spark.read.csv(\"driver.csv\", schema=driver_schema, header=True)\n",
    "restaurant_df = spark.read.csv(\"restaurants.csv\", schema=restaurant_schema, header=True)\n",
    "new_order_df = spark.read.csv(\"new_order.csv\", schema=new_order_schema, header=True)\n",
    "\n",
    "# Verify Schema & Sample Data\n",
    "print(\"Delivery Address Schema:\")\n",
    "delivery_address_df.printSchema()\n",
    "print(\"Sample Data:\")\n",
    "delivery_address_df.show(5)\n",
    "\n",
    "print(\"Driver Schema:\")\n",
    "driver_df.printSchema()\n",
    "print(\"Sample Data:\")\n",
    "driver_df.show(5)\n",
    "\n",
    "print(\"Restaurant Schema:\")\n",
    "restaurant_df.printSchema()\n",
    "print(\"Sample Data:\")\n",
    "restaurant_df.show(5)\n",
    "\n",
    "print(\"New Order Schema:\")\n",
    "new_order_df.printSchema()\n",
    "print(\"Sample Data:\")\n",
    "new_order_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using the Kafka topic (orders) from the producer in Task 1, ingest the streaming data into Spark Streaming, assuming all data comes in the String format. Except for the 'order_ts' column, you shall receive it as an Int type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, floor, rand\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define Schema\n",
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"delivery_person_id\", StringType(), True),\n",
    "    StructField(\"order_ts\", IntegerType(), True),\n",
    "    StructField(\"ready_ts\", StringType(), True),\n",
    "    StructField(\"weather_condition\", StringType(), True),\n",
    "    StructField(\"road_condition\", StringType(), True),\n",
    "    StructField(\"type_of_order\", StringType(), True),\n",
    "    StructField(\"order_total\", StringType(), True),\n",
    "    StructField(\"delivery_time\", StringType(), True),\n",
    "    StructField(\"travel_distance\", StringType(), True),\n",
    "    StructField(\"restaurant_id\", StringType(), True),\n",
    "    StructField(\"delivery_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Reduce Spark logging\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"new_orders\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert Kafka value to string\n",
    "json_stream = kafka_stream.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse JSON into DataFrame\n",
    "orders_df = json_stream.select(from_json(col(\"value\"), order_schema).alias(\"data\")).select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_orders = orders_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .queryName(\"ordes_df\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_orders.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Then, the streaming data frames (orders and drivers) should be transformed into the proper formats following the metadata file schema, similar to assignment 2A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "driver_schema = StructType([\n",
    "    StructField(\"driver_id\", StringType(), True),  \n",
    "    StructField(\"age\", StringType(), True),  \n",
    "    StructField(\"rating\", StringType(), True),\n",
    "    StructField(\"year_experience\",StringType(), True),  \n",
    "    StructField(\"vehicle_condition\", StringType(), True),\n",
    "    StructField(\"type_of_vehicle\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Read drivers\n",
    "kafka_drivers = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"available_drivers\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "kafka_drivers.printSchema()\n",
    "\n",
    "# 4. Parse JSON into Structured DataFrames\n",
    "driver_df = kafka_drivers.select(from_json(col(\"value\").cast(\"string\"), driver_schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "driver_transformed = driver_df \\\n",
    "    .withColumn(\"driver_id\", when(col(\"driver_id\").isNotNull(), col(\"driver_id\").cast(\"int\"))) \\\n",
    "    .withColumn(\"age\", when(col(\"age\").isNotNull(), col(\"age\").cast(\"int\"))) \\\n",
    "    .withColumn(\"rating\", when(col(\"rating\").isNotNull(), col(\"rating\").cast(\"float\"))) \\\n",
    "    .withColumn(\"year_experience\", when(col(\"year_experience\").isNotNull(), col(\"year_experience\").cast(\"int\"))) \\\n",
    "    .withColumn(\"vehicle_condition\", col(\"vehicle_condition\").cast(\"string\")) \\\n",
    "    .withColumn(\"type_of_vehicle\", col(\"type_of_vehicle\").cast(\"string\"))\n",
    "\n",
    "\n",
    "query_drivers1 = driver_transformed.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .queryName(\"driverQuery\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_drivers1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_orders.stop()\n",
    "query_drivers1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Convert `ready_ts` and `order_ts` to TIMESTAMP format\n",
    "orders_df = orders_df \\\n",
    "    .withColumn(\"ready_ts\", to_timestamp(col(\"ready_ts\"))) \\\n",
    "    .withColumn(\"order_ts\", to_timestamp(col(\"order_ts\")))\n",
    "\n",
    "driver_transformed = driver_transformed.withColumn(\"timestamp\", current_timestamp())\n",
    "\n",
    "# Apply Watermarking\n",
    "orders_limited = orders_df.withWatermark(\"ready_ts\", \"60 minutes\").limit(50)\n",
    "drivers_limited = driver_transformed.withWatermark(\"timestamp\", \"60 minutes\").limit(20)\n",
    "        \n",
    "# Write into parquet files the unsuccessful requests partitioned by status code\n",
    "query_orders = orders_limited.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/orders_df\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/orders_df/checkpoint\")\\\n",
    "        .queryName(\"query_oders\")\\\n",
    "        .start()\n",
    "\n",
    "# Write into parquet files the unsuccessful requests partitioned by status code\n",
    "query_drivers = drivers_limited.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/drivers_df\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/drivers_df/checkpoint\")\\\n",
    "        .queryName(\"query_drivers\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_orders.stop()\n",
    "query_drivers.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- delivery_person_id: string (nullable = true)\n",
      " |-- order_ts: timestamp (nullable = true)\n",
      " |-- ready_ts: timestamp (nullable = true)\n",
      " |-- weather_condition: string (nullable = true)\n",
      " |-- road_condition: string (nullable = true)\n",
      " |-- type_of_order: string (nullable = true)\n",
      " |-- order_total: string (nullable = true)\n",
      " |-- delivery_time: string (nullable = true)\n",
      " |-- travel_distance: string (nullable = true)\n",
      " |-- restaurant_id: string (nullable = true)\n",
      " |-- delivery_id: string (nullable = true)\n",
      "\n",
      "+--------------------+------------------+-------------------+--------+-----------------+--------------+-------------+-----------+-------------+---------------+-------------+-----------+\n",
      "|            order_id|delivery_person_id|           order_ts|ready_ts|weather_condition|road_condition|type_of_order|order_total|delivery_time|travel_distance|restaurant_id|delivery_id|\n",
      "+--------------------+------------------+-------------------+--------+-----------------+--------------+-------------+-----------+-------------+---------------+-------------+-----------+\n",
      "|60b91dda-e32e-4e4...|              1597|2025-02-07 11:56:15|    NULL|            Foggy|           Low|       Snacks|         11|         NULL|            9.5|          779|       5542|\n",
      "|ef60a08f-2700-416...|              1558|2025-02-07 11:56:15|    NULL|            Windy|        Medium|       Drinks|          8|         NULL|            4.5|          891|       3092|\n",
      "|b840661a-18b5-436...|              1374|2025-02-07 11:56:15|    NULL|            Foggy|           Jam|         Meal|        175|         NULL|            0.5|          991|       3066|\n",
      "|c8ca16b9-fa0d-473...|              1559|2025-02-07 11:56:15|    NULL|           Stormy|          High|       Drinks|          6|         NULL|            7.5|          806|       9357|\n",
      "|1078284e-b245-4a5...|              1054|2025-02-07 11:56:15|    NULL|            Foggy|        Medium|        Combo|        380|         NULL|            4.5|          539|       7239|\n",
      "|2d071677-aaf5-4e0...|              1989|2025-02-07 11:56:15|    NULL|            Rainy|        Medium|       Snacks|         15|         NULL|            5.5|          111|       2432|\n",
      "|3c0737f1-7970-425...|              1011|2025-02-07 11:56:15|    NULL|            Foggy|          High|       Snacks|         15|         NULL|            0.5|          346|       7669|\n",
      "|00660d1b-3a4b-431...|              1577|2025-02-07 11:56:15|    NULL|            Sunny|          High|      Dessert|         20|         NULL|           10.5|          320|       3632|\n",
      "|d59f173f-a308-472...|              1495|2025-02-07 11:56:15|    NULL|            Foggy|           Jam|       Snacks|         18|         NULL|            6.5|          581|       2050|\n",
      "|8371b49e-3ff8-470...|              1136|2025-02-07 11:56:15|    NULL|           Stormy|           Low|       Drinks|          5|         NULL|            4.5|           47|        773|\n",
      "|46a60a8d-d82a-40b...|              1089|2025-02-07 11:56:16|    NULL|            Foggy|          High|       Drinks|         14|         NULL|            5.5|          734|       9609|\n",
      "|22e59c17-8d1f-463...|              1846|2025-02-07 11:56:16|    NULL|           Stormy|           Low|       Drinks|         12|         NULL|            6.5|          242|       8811|\n",
      "|22e0aaa4-6828-43f...|              1671|2025-02-07 11:56:16|    NULL|           Cloudy|           Low|         Meal|         31|         NULL|            9.5|          112|       4116|\n",
      "|7eaa680f-074a-44c...|              1832|2025-02-07 11:56:16|    NULL|            Windy|           Jam|       Drinks|          8|         NULL|            0.5|          789|        547|\n",
      "|ed8e34a4-ea5e-460...|              1061|2025-02-07 11:56:16|    NULL|           Stormy|           Jam|         Meal|        175|         NULL|            0.5|          954|       6675|\n",
      "|6eae3a7b-df54-442...|              1131|2025-02-07 11:56:16|    NULL|            Foggy|        Medium|        Combo|        389|         NULL|            5.5|          303|       9148|\n",
      "|0bc66fa6-bc23-45f...|              1111|2025-02-07 11:56:16|    NULL|           Stormy|           Jam|      Dessert|         26|         NULL|            1.5|          314|       8887|\n",
      "|2e7b636f-b26a-470...|              1191|2025-02-07 11:56:16|    NULL|            Windy|           Jam|       Drinks|         14|         NULL|            0.5|          851|       6474|\n",
      "|73225522-2a21-484...|              1020|2025-02-07 11:56:16|    NULL|            Windy|           Low|       Snacks|          5|         NULL|            8.5|          452|       5562|\n",
      "|5a83abe5-cbfa-44d...|              1527|2025-02-07 11:56:16|    NULL|            Rainy|          High|      Dessert|         16|         NULL|            1.5|          551|       5746|\n",
      "+--------------------+------------------+-------------------+--------+-----------------+--------------+-------------+-----------+-------------+---------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#orders = spark.read.parquet(\"parquet/orders_df\")\n",
    "orders = spark.read.parquet(\"parquet/orders_df\")\n",
    "orders.printSchema()\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- driver_id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      " |-- year_experience: integer (nullable = true)\n",
      " |-- vehicle_condition: string (nullable = true)\n",
      " |-- type_of_vehicle: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = false)\n",
      "\n",
      "+---------+---+------+---------------+-----------------+---------------+--------------------+\n",
      "|driver_id|age|rating|year_experience|vehicle_condition|type_of_vehicle|           timestamp|\n",
      "+---------+---+------+---------------+-----------------+---------------+--------------------+\n",
      "|     1164| 18|   3.2|              1|             Poor|        Scooter|2025-02-07 11:56:...|\n",
      "|     1575| 35|   3.2|              3|             Poor|          eBike|2025-02-07 11:56:...|\n",
      "|     1158| 29|   3.7|              2|        Excellent|      eSchooter|2025-02-07 11:56:...|\n",
      "|     1556| 56|   3.7|              3|        Excellent|           Bike|2025-02-07 11:56:...|\n",
      "|     1551| 50|   4.1|              5|             Poor|           Bike|2025-02-07 11:56:...|\n",
      "|     1979| 42|   3.2|              1|             Fair|     Motorcycle|2025-02-07 11:56:...|\n",
      "|     1633| 25|   4.3|              1|        Excellent|            Car|2025-02-07 11:56:...|\n",
      "|     1417| 19|   3.9|              4|             Good|            Car|2025-02-07 11:56:...|\n",
      "|     1508| 31|   4.4|              2|             Good|     Motorcycle|2025-02-07 11:56:...|\n",
      "|     1093| 54|   4.8|              1|             Good|          eBike|2025-02-07 11:56:...|\n",
      "|     1454| 35|   3.6|              0|             Good|        Scooter|2025-02-07 11:56:...|\n",
      "|     1209| 51|   4.5|              5|             Good|          eBike|2025-02-07 11:56:...|\n",
      "|     1343| 52|   3.4|              5|        Excellent|            Car|2025-02-07 11:56:...|\n",
      "|     1999| 58|   3.2|              3|             Fair|           Bike|2025-02-07 11:56:...|\n",
      "|     1022| 60|   4.3|              5|        Excellent|      eSchooter|2025-02-07 11:56:...|\n",
      "|     1602| 51|   3.4|              4|        Excellent|          eBike|2025-02-07 11:56:...|\n",
      "|     1181| 58|   4.6|              5|             Fair|           Bike|2025-02-07 11:56:...|\n",
      "|     1614| 45|   3.2|              2|             Good|      eSchooter|2025-02-07 11:56:...|\n",
      "|     1566| 34|   3.3|              4|             Poor|          eBike|2025-02-07 11:56:...|\n",
      "|     1616| 24|   4.1|              2|             Poor|        Scooter|2025-02-07 11:56:...|\n",
      "+---------+---+------+---------------+-----------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "driverss = spark.read.parquet(\"parquet/drivers_df\")\n",
    "driverss.printSchema()\n",
    "driverss.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_orders.stop()\n",
    "#query_drivers.stop()\n",
    "#query_final.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "\n",
    "# Define schema explicitly\n",
    "cross_join_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"delivery_person_id\", StringType(), True),\n",
    "    StructField(\"order_ts\", TimestampType(), True),\n",
    "    StructField(\"ready_ts\", TimestampType(), True),\n",
    "    StructField(\"weather_condition\", StringType(), True),\n",
    "    StructField(\"road_condition\", StringType(), True),\n",
    "    StructField(\"type_of_order\", StringType(), True),\n",
    "    StructField(\"order_total\", IntegerType(), True),  # Ensure it's explicitly read as INTEGER\n",
    "    StructField(\"delivery_time\", IntegerType(), True),\n",
    "    StructField(\"travel_distance\", FloatType(), True),\n",
    "    StructField(\"restaurant_id\", IntegerType(), True),\n",
    "    StructField(\"delivery_id\", StringType(), True),\n",
    "    StructField(\"driver_id\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"rating\", FloatType(), True),\n",
    "    StructField(\"year_experience\", IntegerType(), True),\n",
    "    StructField(\"vehicle_condition\", StringType(), True),\n",
    "    StructField(\"type_of_vehicle\", StringType(), True)\n",
    "])\n",
    "\n",
    "# # Read Parquet with defined schema\n",
    "# cross_join_df = spark.read.schema(cross_join_schema).parquet(\"parquet/orders_drivers_joined\")\n",
    "\n",
    "# # Verify schema\n",
    "# cross_join_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- delivery_person_id: string (nullable = true)\n",
      " |-- order_ts: timestamp (nullable = true)\n",
      " |-- ready_ts: timestamp (nullable = true)\n",
      " |-- weather_condition: string (nullable = true)\n",
      " |-- road_condition: string (nullable = true)\n",
      " |-- type_of_order: string (nullable = true)\n",
      " |-- order_total: integer (nullable = true)\n",
      " |-- delivery_time: integer (nullable = true)\n",
      " |-- travel_distance: float (nullable = true)\n",
      " |-- restaurant_id: integer (nullable = true)\n",
      " |-- delivery_id: string (nullable = true)\n",
      " |-- driver_id: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      " |-- year_experience: integer (nullable = true)\n",
      " |-- vehicle_condition: string (nullable = true)\n",
      " |-- type_of_vehicle: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o605.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2805.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2805.0 (TID 4207) (d3e6dbbe977f executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file file:///home/student/parquet/orders_drivers_joined/part-00000-f7f0db1b-0380-4089-a825-e96c70f2dc5d-c000.snappy.parquet. Column: [delivery_time], Expected: int, Found: BINARY.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [delivery_time], physicalType: BINARY, logicalType: int\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1127)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:189)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Parquet column cannot be converted in file file:///home/student/parquet/orders_drivers_joined/part-00000-f7f0db1b-0380-4089-a825-e96c70f2dc5d-c000.snappy.parquet. Column: [delivery_time], Expected: int, Found: BINARY.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [delivery_time], physicalType: BINARY, logicalType: int\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1127)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:189)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 23 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m cross_join_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mschema(cross_join_schema)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet/orders_drivers_joined\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m cross_join_df\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mcross_join_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o605.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2805.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2805.0 (TID 4207) (d3e6dbbe977f executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file file:///home/student/parquet/orders_drivers_joined/part-00000-f7f0db1b-0380-4089-a825-e96c70f2dc5d-c000.snappy.parquet. Column: [delivery_time], Expected: int, Found: BINARY.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [delivery_time], physicalType: BINARY, logicalType: int\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1127)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:189)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Parquet column cannot be converted in file file:///home/student/parquet/orders_drivers_joined/part-00000-f7f0db1b-0380-4089-a825-e96c70f2dc5d-c000.snappy.parquet. Column: [delivery_time], Expected: int, Found: BINARY.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [delivery_time], physicalType: BINARY, logicalType: int\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1127)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:189)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "# Below cross join is working but very very slowly.\n",
    "\n",
    "# Step 1: Read Orders and Drivers from Parquet (Batch Mode)\n",
    "orders_batch = spark.read.parquet(\"parquet/orders_df\")\n",
    "drivers_batch = spark.read.parquet(\"parquet/drivers_df\")\n",
    "\n",
    "# Step 2: Perform Cross Join\n",
    "cross_join_df = orders_batch.crossJoin(drivers_batch)\n",
    "# Step 3: Explicitly cast order_total to IntegerType (to prevent issues)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "cross_join_df = cross_join_df.withColumn(\"order_total\", col(\"order_total\").cast(\"int\"))\n",
    "\n",
    "# Step 3: Write the Cross Join Result to a New Parquet File\n",
    "cross_join_df.write.mode(\"append\").parquet(\"parquet/orders_drivers_joined\")\n",
    "\n",
    "# Step 4: Display Output\n",
    "#cross_join_df.show(truncate=False)\n",
    "\n",
    "# Load the Cross Joined Data\n",
    "cross_join_df = spark.read.schema(cross_join_schema).parquet(\"parquet/orders_drivers_joined\")\n",
    "cross_join_df.printSchema()\n",
    "cross_join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\tFrom each order, a) select 5 random drivers; b) use your ML model to predict their delivery time; c) select the fastest driver (i.e. the shortest delivery time), assign the driver to the order and then update delivery_time with your prediction.  \n",
    "•\tNote 1: You may need to join other data frames like restaurant and delivery_address if you used them in your model.  \n",
    "•\tNote 2: Assume one driver can only carry one order at a time within a batch. Your “random 5” selection should exclude those drivers who have already been assigned to an order.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)\n",
    "from pyspark.sql.functions import rand, collect_list, explode, slice\n",
    "\n",
    "# Select 5 random drivers per order\n",
    "random_drivers = cross_join_df.withColumn(\"rand\", rand()) \\\n",
    "    .orderBy(\"order_id\", \"rand\") \\\n",
    "    .groupby(\"order_id\") \\\n",
    "    .agg(collect_list(\"driver_id\").alias(\"drivers\")) \\\n",
    "    .withColumn(\"drivers\", slice(col(\"drivers\"), 1, 5))\n",
    "\n",
    "# Explode to get one row per driver\n",
    "random_drivers = random_drivers.select(\"order_id\", explode(\"drivers\").alias(\"driver_id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# b)\n",
    "from pyspark.sql.functions import col, rand, collect_list, explode, slice\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Load the pretrained model\n",
    "model = PipelineModel.load(\"best_model\")\n",
    "\n",
    "# Load required data\n",
    "orders_df = spark.read.parquet(\"parquet/orders_df\")\n",
    "drivers_df = spark.read.parquet(\"parquet/drivers_df\")\n",
    "\n",
    "#delivery_address_df = spark.read.parquet(\"delivery_address.parquet\")\n",
    "#estaurants_df = spark.read.parquet(\"restaurants.parquet\")\n",
    "\n",
    "# Load the Cross Joined Data\n",
    "cross_join_df = spark.read.schema(cross_join_schema).parquet(\"parquet/orders_drivers_joined\")\n",
    "\n",
    "# Select 5 random drivers per order\n",
    "random_drivers = cross_join_df.withColumn(\"rand\", rand()) \\\n",
    "    .orderBy(\"order_id\", \"rand\") \\\n",
    "    .groupby(\"order_id\") \\\n",
    "    .agg(collect_list(\"driver_id\").alias(\"drivers\")) \\\n",
    "    .withColumn(\"drivers\", slice(col(\"drivers\"), 1, 5))\n",
    "\n",
    "# Explode to get one row per driver\n",
    "random_drivers = random_drivers.select(\"order_id\", explode(\"drivers\").alias(\"driver_id\"))\n",
    "\n",
    "# Predict delivery time for selected drivers\n",
    "predictions = model.transform(random_drivers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\tPerform the following aggregations:  \n",
    "a)\tEvery 15 seconds, show the total number of revenue (sum of order_total) for each type of order (drinks, meals, snacks, etc.).   \n",
    "b)\tEvery 30 seconds, for each suburb of restaurants, count the number of orders with predicted delivery time <=15 minutes and > 15 minutes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6a)\n",
    "from pyspark.sql.functions import window, sum, col\n",
    "\n",
    "# Define the path to the incoming Parquet files\n",
    "parquet_path = \"parquet/orders_drivers_joined\"\n",
    "\n",
    "# Read Parquet files as a Streaming DataFrame\n",
    "cross_join_df = spark.readStream \\\n",
    "    .schema(cross_join_df.schema) \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", parquet_path) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure order_total is an integer\n",
    "cross_join_df = cross_join_df.withColumn(\"order_total\", col(\"order_total\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add watermark for handling late data\n",
    "cross_join_df = cross_join_df.withWatermark(\"order_ts\", \"15 minutes\")\n",
    "\n",
    "# Aggregate revenue every 15 seconds for each type of order\n",
    "time_windowed_revenue = cross_join_df.withColumn(\"time_window\", window(col(\"order_ts\"), \"15 seconds\")) \\\n",
    "    .groupby(\"time_window\", \"type_of_order\") \\\n",
    "    .agg(sum(\"order_total\").alias(\"total_revenue\"))\n",
    "\n",
    "# Output the results to the console in Update Mode (required for windowed aggregations)\n",
    "query = time_windowed_revenue.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .queryName(\"cross_join\")\\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "# This query takes little long to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b\n",
    "from pyspark.sql.functions import count, when\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "cross_join_df = cross_join_df.withColumn(\"ready_ts\", from_unixtime(col(\"ready_ts\") / 1000).cast(\"timestamp\"))\n",
    "\n",
    "#  Perform the aggregation every 30 seconds\n",
    "delivery_time_aggregation = cross_join_df \\\n",
    "    .groupBy(\n",
    "        window(col(\"ready_ts\"), \"30 seconds\"),  # 30-second time window\n",
    "        col(\"restaurant_suburb\")  # Assuming restaurant data has 'restaurant_suburb'\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(when(col(\"predicted_delivery_time\") <= 15, True)).alias(\"count_under_15\"),\n",
    "        count(when(col(\"predicted_delivery_time\") > 15, True)).alias(\"count_over_15\")\n",
    "    ) \\\n",
    "    .select(\"window\", \"restaurant_suburb\", \"count_under_15\", \"count_over_15\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\tSave the data from 6a and 6b to a Parquet file as streams. (Hint: Parquet files support streaming writing/reading. The file keeps updating while new batches arrive.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7a(save 6a)\n",
    "# Streaming Write for Task 6a (Revenue per Order Type)\n",
    "query_revenue = time_windowed_revenue.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoints/revenue\") \\\n",
    "    .option(\"path\", \"parquet/time_windowed_revenue\") \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_revenue.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7b(save 6b)\n",
    "# Task 6b: Compute Delivery Time Categories per Restaurant Suburb Every 30 Seconds\n",
    "# Write into parquet files the unsuccessful requests partitioned by status code\n",
    "delivery_agg_query = delivery_time_aggregation.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquetdelivery_time_aggregation\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/delivery_time_aggregation/checkpoint\")\\\n",
    "        .queryName(\"query_aggre\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_agg_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Read the two parquet files from task 7 as data streams and send them to Kafka topics with appropriate names.  \n",
    "(Note: You shall read the parquet files as a streaming data frame and send messages to the Kafka topic when new data appears in the parquet file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7fa8d0d7a290>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_bootstrap_servers = \"kafka:9092\"  \n",
    "kafka_topic_6a = \"time_windowed_revenue\"  \n",
    "kafka_topic_6b = \"driver_performance\"     \n",
    "\n",
    "schema_6a = StructType([\n",
    "    StructField(\"time_window\", StructType([  # Nested structure for window\n",
    "        StructField(\"start\", TimestampType(), True),  # Window start time\n",
    "        StructField(\"end\", TimestampType(), True)  # Window end time\n",
    "    ]), True),\n",
    "    StructField(\"type_of_order\", StringType(), True),  # Order category\n",
    "    StructField(\"total_revenue\", IntegerType(), True)  # Sum of revenue in the window\n",
    "])\n",
    "\n",
    "# Read the two parquet files as streaming data frames\n",
    "time_windowed_revenue_stream = spark.readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .schema(schema_6a) \\\n",
    "    .option(\"path\", \"parquet/time_windowed_revenue\") \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "# Send the data to Kafka\n",
    "time_windowed_revenue_stream.selectExpr(\"to_json(struct(*)) AS value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"time_windowed_revenue_topic\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint/time_windowed_revenue_kafka\") \\\n",
    "    .start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
